{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-09T08:13:09.745891Z","iopub.status.busy":"2024-09-09T08:13:09.745546Z","iopub.status.idle":"2024-09-09T08:13:10.205546Z","shell.execute_reply":"2024-09-09T08:13:10.204481Z","shell.execute_reply.started":"2024-09-09T08:13:09.745845Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{},"source":["## 2. **Topic Modeling**\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-09-09T08:45:06.366947Z","iopub.status.busy":"2024-09-09T08:45:06.366464Z","iopub.status.idle":"2024-09-09T08:45:18.050397Z","shell.execute_reply":"2024-09-09T08:45:18.049069Z","shell.execute_reply.started":"2024-09-09T08:45:06.366905Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import gensim\n","from gensim import corpora\n","from nltk.corpus import stopwords\n","import nltk\n","from joblib import Parallel, delayed\n","import re\n","\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-09T08:45:27.212937Z","iopub.status.busy":"2024-09-09T08:45:27.211762Z","iopub.status.idle":"2024-09-09T08:49:51.087069Z","shell.execute_reply":"2024-09-09T08:49:51.085824Z","shell.execute_reply.started":"2024-09-09T08:45:27.212880Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting preprocessing...\n","Processing chunk 0 to 100000 of 2972448\n","Processing chunk 100000 to 200000 of 2972448\n","Processing chunk 200000 to 300000 of 2972448\n","Processing chunk 300000 to 400000 of 2972448\n","Processing chunk 400000 to 500000 of 2972448\n","Processing chunk 500000 to 600000 of 2972448\n","Processing chunk 600000 to 700000 of 2972448\n","Processing chunk 700000 to 800000 of 2972448\n","Processing chunk 800000 to 900000 of 2972448\n","Processing chunk 900000 to 1000000 of 2972448\n","Processing chunk 1000000 to 1100000 of 2972448\n","Processing chunk 1100000 to 1200000 of 2972448\n","Processing chunk 1200000 to 1300000 of 2972448\n","Processing chunk 1300000 to 1400000 of 2972448\n","Processing chunk 1400000 to 1500000 of 2972448\n","Processing chunk 1500000 to 1600000 of 2972448\n","Processing chunk 1600000 to 1700000 of 2972448\n","Processing chunk 1700000 to 1800000 of 2972448\n","Processing chunk 1800000 to 1900000 of 2972448\n","Processing chunk 1900000 to 2000000 of 2972448\n","Processing chunk 2000000 to 2100000 of 2972448\n","Processing chunk 2100000 to 2200000 of 2972448\n","Processing chunk 2200000 to 2300000 of 2972448\n","Processing chunk 2300000 to 2400000 of 2972448\n","Processing chunk 2400000 to 2500000 of 2972448\n","Processing chunk 2500000 to 2600000 of 2972448\n","Processing chunk 2600000 to 2700000 of 2972448\n","Processing chunk 2700000 to 2800000 of 2972448\n","Processing chunk 2800000 to 2900000 of 2972448\n","Processing chunk 2900000 to 2972448 of 2972448\n","Preprocessing completed.\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Lemmatized review/text</th>\n","      <th>tokens</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>julie strain fans collection photo page worth ...</td>\n","      <td>[julie, strain, fans, collection, photo, page,...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>do not care much dr seuss reading philip nel b...</td>\n","      <td>[care, much, dr, seuss, reading, philip, nel, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>people become book read child father man dr se...</td>\n","      <td>[people, become, book, read, child, father, ma...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>theodore seuss geisel aka quotdr seussquot one...</td>\n","      <td>[theodore, seuss, geisel, aka, quotdr, seussqu...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>philip nel dr seuss american iconthis basicall...</td>\n","      <td>[philip, nel, dr, seuss, american, iconthis, b...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                              Lemmatized review/text  \\\n","0  julie strain fans collection photo page worth ...   \n","1  do not care much dr seuss reading philip nel b...   \n","2  people become book read child father man dr se...   \n","3  theodore seuss geisel aka quotdr seussquot one...   \n","4  philip nel dr seuss american iconthis basicall...   \n","\n","                                              tokens  \n","0  [julie, strain, fans, collection, photo, page,...  \n","1  [care, much, dr, seuss, reading, philip, nel, ...  \n","2  [people, become, book, read, child, father, ma...  \n","3  [theodore, seuss, geisel, aka, quotdr, seussqu...  \n","4  [philip, nel, dr, seuss, american, iconthis, b...  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Load stopwords\n","stop_words = set(stopwords.words('english'))\n","\n","# Define preprocessing function\n","def preprocess_for_lda(text):\n","    # Handle non-string entries by converting to empty string if necessary\n","    if not isinstance(text, str):\n","        text = ''\n","    \n","    # Tokenize and remove stopwords\n","    tokens = text.lower().split()\n","    tokens = [word for word in tokens if word not in stop_words]\n","    \n","    # Optionally, clean tokens further (remove punctuation, etc.)\n","    tokens = [re.sub(r'\\W+', '', token) for token in tokens]  # Remove special characters\n","    return tokens\n","\n","# Parallelize the preprocessing step in chunks for better progress tracking\n","def process_texts_parallel(texts, chunk_size=100000):\n","    num_cores = -1  # Use all available cores\n","    total_texts = len(texts)\n","    processed_texts = []\n","    \n","    print(\"Starting preprocessing...\")\n","    for start in range(0, total_texts, chunk_size):\n","        end = min(start + chunk_size, total_texts)\n","        print(f\"Processing chunk {start} to {end} of {total_texts}\")\n","        \n","        # Parallel processing of each chunk\n","        chunk_processed = Parallel(n_jobs=num_cores)(\n","            delayed(preprocess_for_lda)(text) for text in texts[start:end]\n","        )\n","        \n","        # Append the processed chunk to the result list\n","        processed_texts.extend(chunk_processed)\n","    \n","    print(\"Preprocessing completed.\")\n","    return processed_texts\n","\n","# Apply preprocessing\n","df_cleaned_rating['tokens'] = process_texts_parallel(df_cleaned_rating['Lemmatized review/text'])\n","\n","df_cleaned_rating[['Lemmatized review/text', 'tokens']].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-09T08:50:03.059291Z","iopub.status.busy":"2024-09-09T08:50:03.058811Z"},"trusted":true},"outputs":[],"source":["from gensim import corpora\n","\n","# Create a dictionary representation of the documents (tokens)\n","dictionary = corpora.Dictionary(df_cleaned_rating['tokens'])\n","\n","# Optionally, filter extremes to remove very common or rare tokens\n","dictionary.filter_extremes(no_below=5, no_above=0.5)  # Adjust thresholds as needed"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-07T18:44:38.647007Z","iopub.status.busy":"2024-09-07T18:44:38.646010Z","iopub.status.idle":"2024-09-07T18:44:38.740944Z","shell.execute_reply":"2024-09-07T18:44:38.739767Z","shell.execute_reply.started":"2024-09-07T18:44:38.646954Z"},"trusted":true},"outputs":[],"source":["# Save dictionary for future use\n","dictionary.save('dictionary.dict')"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-07T18:44:42.805456Z","iopub.status.busy":"2024-09-07T18:44:42.804955Z","iopub.status.idle":"2024-09-07T18:54:09.807666Z","shell.execute_reply":"2024-09-07T18:54:09.806454Z","shell.execute_reply.started":"2024-09-07T18:44:42.805410Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing chunk 0 to 100000\n","Saved corpus chunk 0 to 100000 to corpus_chunks/corpus_chunk_0.mm\n","Processing chunk 100000 to 200000\n","Saved corpus chunk 100000 to 200000 to corpus_chunks/corpus_chunk_1.mm\n","Processing chunk 200000 to 300000\n","Saved corpus chunk 200000 to 300000 to corpus_chunks/corpus_chunk_2.mm\n","Processing chunk 300000 to 400000\n","Saved corpus chunk 300000 to 400000 to corpus_chunks/corpus_chunk_3.mm\n","Processing chunk 400000 to 500000\n","Saved corpus chunk 400000 to 500000 to corpus_chunks/corpus_chunk_4.mm\n","Processing chunk 500000 to 600000\n","Saved corpus chunk 500000 to 600000 to corpus_chunks/corpus_chunk_5.mm\n","Processing chunk 600000 to 700000\n","Saved corpus chunk 600000 to 700000 to corpus_chunks/corpus_chunk_6.mm\n","Processing chunk 700000 to 800000\n","Saved corpus chunk 700000 to 800000 to corpus_chunks/corpus_chunk_7.mm\n","Processing chunk 800000 to 900000\n","Saved corpus chunk 800000 to 900000 to corpus_chunks/corpus_chunk_8.mm\n","Processing chunk 900000 to 1000000\n","Saved corpus chunk 900000 to 1000000 to corpus_chunks/corpus_chunk_9.mm\n","Processing chunk 1000000 to 1100000\n","Saved corpus chunk 1000000 to 1100000 to corpus_chunks/corpus_chunk_10.mm\n","Processing chunk 1100000 to 1200000\n","Saved corpus chunk 1100000 to 1200000 to corpus_chunks/corpus_chunk_11.mm\n","Processing chunk 1200000 to 1300000\n","Saved corpus chunk 1200000 to 1300000 to corpus_chunks/corpus_chunk_12.mm\n","Processing chunk 1300000 to 1400000\n","Saved corpus chunk 1300000 to 1400000 to corpus_chunks/corpus_chunk_13.mm\n","Processing chunk 1400000 to 1500000\n","Saved corpus chunk 1400000 to 1500000 to corpus_chunks/corpus_chunk_14.mm\n","Processing chunk 1500000 to 1600000\n","Saved corpus chunk 1500000 to 1600000 to corpus_chunks/corpus_chunk_15.mm\n","Processing chunk 1600000 to 1700000\n","Saved corpus chunk 1600000 to 1700000 to corpus_chunks/corpus_chunk_16.mm\n","Processing chunk 1700000 to 1800000\n","Saved corpus chunk 1700000 to 1800000 to corpus_chunks/corpus_chunk_17.mm\n","Processing chunk 1800000 to 1900000\n","Saved corpus chunk 1800000 to 1900000 to corpus_chunks/corpus_chunk_18.mm\n","Processing chunk 1900000 to 2000000\n","Saved corpus chunk 1900000 to 2000000 to corpus_chunks/corpus_chunk_19.mm\n","Processing chunk 2000000 to 2100000\n","Saved corpus chunk 2000000 to 2100000 to corpus_chunks/corpus_chunk_20.mm\n","Processing chunk 2100000 to 2200000\n","Saved corpus chunk 2100000 to 2200000 to corpus_chunks/corpus_chunk_21.mm\n","Processing chunk 2200000 to 2300000\n","Saved corpus chunk 2200000 to 2300000 to corpus_chunks/corpus_chunk_22.mm\n","Processing chunk 2300000 to 2400000\n","Saved corpus chunk 2300000 to 2400000 to corpus_chunks/corpus_chunk_23.mm\n","Processing chunk 2400000 to 2500000\n","Saved corpus chunk 2400000 to 2500000 to corpus_chunks/corpus_chunk_24.mm\n","Processing chunk 2500000 to 2600000\n","Saved corpus chunk 2500000 to 2600000 to corpus_chunks/corpus_chunk_25.mm\n","Processing chunk 2600000 to 2700000\n","Saved corpus chunk 2600000 to 2700000 to corpus_chunks/corpus_chunk_26.mm\n","Processing chunk 2700000 to 2800000\n","Saved corpus chunk 2700000 to 2800000 to corpus_chunks/corpus_chunk_27.mm\n","Processing chunk 2800000 to 2900000\n","Saved corpus chunk 2800000 to 2900000 to corpus_chunks/corpus_chunk_28.mm\n","Processing chunk 2900000 to 2972448\n","Saved corpus chunk 2900000 to 2972448 to corpus_chunks/corpus_chunk_29.mm\n","Corpus creation in chunks completed.\n"]}],"source":["import os\n","\n","# Define chunk size (number of documents per chunk)\n","chunk_size = 100000  # Adjust based on memory limits\n","\n","# Initialize an empty list to collect corpus chunks or save to disk directly\n","corpus_chunks = []\n","\n","# Create a directory to save the corpus chunks if it doesn't exist\n","if not os.path.exists('corpus_chunks'):\n","    os.makedirs('corpus_chunks')\n","\n","# Process the dataset in chunks\n","for start in range(0, len(df_cleaned_rating), chunk_size):\n","    end = min(start + chunk_size, len(df_cleaned_rating))\n","    \n","    print(f\"Processing chunk {start} to {end}\")\n","    \n","    # Create bag-of-words representation (doc2bow) for the current chunk\n","    corpus_chunk = [dictionary.doc2bow(tokens) for tokens in df_cleaned_rating['tokens'][start:end]]\n","    \n","    # Save the chunk to disk\n","    chunk_filename = f'corpus_chunks/corpus_chunk_{start // chunk_size}.mm'\n","    corpora.MmCorpus.serialize(chunk_filename, corpus_chunk)\n","    \n","    print(f\"Saved corpus chunk {start} to {end} to {chunk_filename}\")\n","\n","print(\"Corpus creation in chunks completed.\")"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-09-07T18:54:24.174587Z","iopub.status.busy":"2024-09-07T18:54:24.173289Z","iopub.status.idle":"2024-09-07T19:24:53.159817Z","shell.execute_reply":"2024-09-07T19:24:53.158492Z","shell.execute_reply.started":"2024-09-07T18:54:24.174532Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing corpus_chunk_0.mm for LDA training...\n","Updated model with corpus_chunk_0.mm\n","Processing corpus_chunk_1.mm for LDA training...\n","Updated model with corpus_chunk_1.mm\n","Processing corpus_chunk_10.mm for LDA training...\n","Updated model with corpus_chunk_10.mm\n","Processing corpus_chunk_11.mm for LDA training...\n","Updated model with corpus_chunk_11.mm\n","Processing corpus_chunk_12.mm for LDA training...\n","Updated model with corpus_chunk_12.mm\n","Processing corpus_chunk_13.mm for LDA training...\n","Updated model with corpus_chunk_13.mm\n","Processing corpus_chunk_14.mm for LDA training...\n","Updated model with corpus_chunk_14.mm\n","Processing corpus_chunk_15.mm for LDA training...\n","Updated model with corpus_chunk_15.mm\n","Processing corpus_chunk_16.mm for LDA training...\n","Updated model with corpus_chunk_16.mm\n","Processing corpus_chunk_17.mm for LDA training...\n","Updated model with corpus_chunk_17.mm\n","Processing corpus_chunk_18.mm for LDA training...\n","Updated model with corpus_chunk_18.mm\n","Processing corpus_chunk_19.mm for LDA training...\n","Updated model with corpus_chunk_19.mm\n","Processing corpus_chunk_2.mm for LDA training...\n","Updated model with corpus_chunk_2.mm\n","Processing corpus_chunk_20.mm for LDA training...\n","Updated model with corpus_chunk_20.mm\n","Processing corpus_chunk_21.mm for LDA training...\n","Updated model with corpus_chunk_21.mm\n","Processing corpus_chunk_22.mm for LDA training...\n","Updated model with corpus_chunk_22.mm\n","Processing corpus_chunk_23.mm for LDA training...\n","Updated model with corpus_chunk_23.mm\n","Processing corpus_chunk_24.mm for LDA training...\n","Updated model with corpus_chunk_24.mm\n","Processing corpus_chunk_25.mm for LDA training...\n","Updated model with corpus_chunk_25.mm\n","Processing corpus_chunk_26.mm for LDA training...\n","Updated model with corpus_chunk_26.mm\n","Processing corpus_chunk_27.mm for LDA training...\n","Updated model with corpus_chunk_27.mm\n","Processing corpus_chunk_28.mm for LDA training...\n","Updated model with corpus_chunk_28.mm\n","Processing corpus_chunk_29.mm for LDA training...\n","Updated model with corpus_chunk_29.mm\n","Processing corpus_chunk_3.mm for LDA training...\n","Updated model with corpus_chunk_3.mm\n","Processing corpus_chunk_4.mm for LDA training...\n","Updated model with corpus_chunk_4.mm\n","Processing corpus_chunk_5.mm for LDA training...\n","Updated model with corpus_chunk_5.mm\n","Processing corpus_chunk_6.mm for LDA training...\n","Updated model with corpus_chunk_6.mm\n","Processing corpus_chunk_7.mm for LDA training...\n","Updated model with corpus_chunk_7.mm\n","Processing corpus_chunk_8.mm for LDA training...\n","Updated model with corpus_chunk_8.mm\n","Processing corpus_chunk_9.mm for LDA training...\n","Updated model with corpus_chunk_9.mm\n","LDA model training completed and saved.\n","Topic 1: 0.029*\"life\" + 0.016*\"love\" + 0.013*\"woman\" + 0.012*\"man\" + 0.011*\"people\" + 0.011*\"child\" + 0.010*\"live\" + 0.009*\"family\" + 0.008*\"young\" + 0.007*\"make\"\n","Topic 2: 0.007*\"work\" + 0.007*\"use\" + 0.005*\"author\" + 0.005*\"many\" + 0.005*\"one\" + 0.005*\"history\" + 0.004*\"well\" + 0.004*\"also\" + 0.004*\"write\" + 0.004*\"provide\"\n","Topic 3: 0.030*\"story\" + 0.029*\"character\" + 0.024*\"novel\" + 0.012*\"one\" + 0.011*\"reader\" + 0.010*\"write\" + 0.009*\"plot\" + 0.008*\"series\" + 0.007*\"end\" + 0.006*\"first\"\n","Topic 4: 0.011*\"war\" + 0.006*\"man\" + 0.005*\"mr\" + 0.005*\"austen\" + 0.005*\"american\" + 0.004*\"year\" + 0.004*\"name\" + 0.004*\"two\" + 0.004*\"take\" + 0.004*\"new\"\n","Topic 5: 0.018*\"like\" + 0.017*\"good\" + 0.017*\"get\" + 0.016*\"one\" + 0.014*\"would\" + 0.012*\"time\" + 0.012*\"think\" + 0.011*\"great\" + 0.011*\"well\" + 0.011*\"make\"\n"]}],"source":["from gensim.models.ldamodel import LdaModel\n","from gensim.corpora import Dictionary\n","import os\n","\n","# Define the number of topics you want to extract\n","num_topics = 5  # Adjust this based on your needs\n","\n","# Initialize the LDA model without training it yet\n","lda_model = LdaModel(id2word=dictionary, \n","                     num_topics=num_topics, \n","                     random_state=42, \n","                     alpha='auto', \n","                     eta='auto')\n","\n","# Load and process the corpus chunks incrementally\n","for chunk_file in sorted(os.listdir('corpus_chunks')):\n","    if chunk_file.endswith('.mm'):\n","        print(f\"Processing {chunk_file} for LDA training...\")\n","        \n","        # Load the chunk\n","        chunk_corpus = corpora.MmCorpus(os.path.join('corpus_chunks', chunk_file))\n","        \n","        # Update the LDA model with the chunk\n","        lda_model.update(chunk_corpus)\n","        \n","        print(f\"Updated model with {chunk_file}\")\n","\n","# After processing all chunks, save the trained LDA model\n","lda_model.save('lda_model_trained.model')\n","print(\"LDA model training completed and saved.\")\n","\n","# Print the top 10 words for each topic\n","for i, topic in lda_model.print_topics(num_topics=num_topics, num_words=10):\n","    print(f\"Topic {i+1}: {topic}\")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-09-07T19:27:07.718963Z","iopub.status.busy":"2024-09-07T19:27:07.718429Z","iopub.status.idle":"2024-09-07T19:53:18.666896Z","shell.execute_reply":"2024-09-07T19:53:18.665487Z","shell.execute_reply.started":"2024-09-07T19:27:07.718918Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dominant topics assigned and saved to DataFrame.\n"]}],"source":["# Load the trained LDA model\n","lda_model = LdaModel.load('lda_model_trained.model')\n","\n","# Define a function to assign the dominant topic\n","def assign_dominant_topic(lda_model, corpus):\n","    topics = [max(lda_model[doc], key=lambda x: x[1])[0] for doc in corpus]\n","    return topics\n","\n","# Load the corpus chunks\n","corpus_chunks = [corpora.MmCorpus(os.path.join('corpus_chunks', file)) for file in sorted(os.listdir('corpus_chunks')) if file.endswith('.mm')]\n","\n","# Initialize an empty list to collect dominant topics\n","dominant_topics = []\n","\n","for chunk_corpus in corpus_chunks:\n","    dominant_topics.extend(assign_dominant_topic(lda_model, chunk_corpus))\n","\n","# Add dominant topics to the DataFrame\n","df_cleaned_rating['Dominant_Topic'] = dominant_topics\n","\n","# Save the DataFrame with dominant topics\n","df_cleaned_rating.to_csv('amazon_books_reviews_with_topics.csv', index=False)\n","print(\"Dominant topics assigned and saved to DataFrame.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-07T20:05:21.849276Z","iopub.status.busy":"2024-09-07T20:05:21.847856Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading corpus_chunk_0.mm for visualization...\n","Loading corpus_chunk_1.mm for visualization...\n","Loading corpus_chunk_10.mm for visualization...\n","Loading corpus_chunk_11.mm for visualization...\n","Loading corpus_chunk_12.mm for visualization...\n","Loading corpus_chunk_13.mm for visualization...\n","Loading corpus_chunk_14.mm for visualization...\n","Loading corpus_chunk_15.mm for visualization...\n","Loading corpus_chunk_16.mm for visualization...\n","Loading corpus_chunk_17.mm for visualization...\n","Loading corpus_chunk_18.mm for visualization...\n","Loading corpus_chunk_19.mm for visualization...\n","Loading corpus_chunk_2.mm for visualization...\n","Loading corpus_chunk_20.mm for visualization...\n","Loading corpus_chunk_21.mm for visualization...\n","Loading corpus_chunk_22.mm for visualization...\n","Loading corpus_chunk_23.mm for visualization...\n","Loading corpus_chunk_24.mm for visualization...\n","Loading corpus_chunk_25.mm for visualization...\n"]}],"source":["from gensim import corpora\n","import os\n","import pyLDAvis.gensim_models as gensimvis\n","import pyLDAvis\n","\n","# Combine all corpus chunks into a single corpus\n","corpus = []\n","for chunk_file in sorted(os.listdir('corpus_chunks')):\n","    if chunk_file.endswith('.mm'):\n","        print(f\"Loading {chunk_file} for visualization...\")\n","        chunk_corpus = corpora.MmCorpus(os.path.join('corpus_chunks', chunk_file))\n","        corpus.extend(chunk_corpus)\n","\n","# Prepare the visualization\n","lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n","\n","# Display the visualization\n","pyLDAvis.display(lda_vis)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5654631,"sourceId":9332327,"sourceType":"datasetVersion"}],"dockerImageVersionId":30761,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
